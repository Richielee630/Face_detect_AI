{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0006674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "img_path_female = './Dataset/TrainFemale_someof/'\n",
    "img_path_male = './Dataset/Train/Male_someof/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ba3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def facecrop(image_arr, image_names, save_path):\n",
    "\n",
    "    facedata = \"haarcascade_frontalface_default.xml\"\n",
    "    cascade = cv2.CascadeClassifier(cv2.data.haarcascades+facedata)\n",
    "    \n",
    "    name_index = 0\n",
    "    for img_data in image_arr:\n",
    "        #img = plt.imread(image)\n",
    "        \n",
    "        minisize = (img_data.shape[1],img_data.shape[0])\n",
    "        miniframe = cv2.resize(img_data, minisize)\n",
    "\n",
    "        faces = cascade.detectMultiScale(miniframe)\n",
    "        \n",
    "        for f in faces:\n",
    "            x, y, w, h = [ v for v in f ]\n",
    "            #cv2.rectangle(img_data, (x,y), (x+w,y+h), (255,255,255))\n",
    "            sub_face = img_data[y:y+h, x:x+w]\n",
    "            #f_base_name = os. path. basename(image)\n",
    "            fname, ext = os.path.splitext(image_names[name_index])\n",
    "            #cv2.imwrite(fname+\"_cropped_\"+str(counter)+ext, sub_face)\n",
    "            cv2.imwrite(os.path.join(save_path , fname+\"_cropped_\"+ext), \n",
    "                        cv2.cvtColor(sub_face, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "        name_index += 1\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916ebdc",
   "metadata": {},
   "source": [
    "## find all female faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e626f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_img_arr = []\n",
    "img_names = []\n",
    "\n",
    "for img in os.listdir(img_path_female):\n",
    "    train_imgpath = os.path.join(img_path_female, img) #train_imgpath = './xxx/xxx/xxx.jpg'\n",
    "    train_img = plt.imread(train_imgpath)\n",
    "    try:\n",
    "        #train_img = cv2.resize(train_img,(144,144)) #**very impoertant step\n",
    "        train_image = np.array(train_img)#.flatten()\n",
    "        female_img_arr.append(train_image)\n",
    "        img_names.append(img)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "np_female_img_arr = np.array(female_img_arr)\n",
    "np_img_names = np.array(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd520d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_female_img_arr[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c0f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Dataset/Train/Female_face_100/'\n",
    "facecrop(np_female_img_arr,np_img_names,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bb509",
   "metadata": {},
   "source": [
    "## find all male faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_img_arr = []\n",
    "img_names_males = []\n",
    "\n",
    "for img in os.listdir(img_path_male):\n",
    "    train_imgpath = os.path.join(img_path_male, img) #train_imgpath = './xxx/xxx/xxx.jpg'\n",
    "    train_img = plt.imread(train_imgpath)\n",
    "    try:\n",
    "        #train_img = cv2.resize(train_img,(144,144)) #**very impoertant step\n",
    "        train_image = np.array(train_img)#.flatten()\n",
    "        male_img_arr.append(train_image)\n",
    "        img_names_males.append(img)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "np_male_img_arr = np.array(male_img_arr)\n",
    "np_img_names_males = np.array(img_names_males)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788df8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10080, 218, 178, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_male_img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adde72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Dataset/Train/Male_face_100/'\n",
    "facecrop(np_male_img_arr,np_img_names_males,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b170090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba2bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4942bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a99859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def facecrop(image):\n",
    "#     save_path = './Female_face_100/'\n",
    "#     sub_faces = []\n",
    "    \n",
    "#     facedata = sys.argv[1]\n",
    "#     cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#     img = plt.imread(image) \n",
    "#     #Note: In the case of color images, the decoded images will have the channels stored in B G R order.\n",
    "#     #print(img.shape[1],img.shape[0])\n",
    "    \n",
    "#     minisize = (img.shape[1],img.shape[0])\n",
    "#     miniframe = cv2.resize(img, minisize)\n",
    "\n",
    "#     faces = cascade.detectMultiScale(miniframe)\n",
    "#     counter = 0\n",
    "#     for f in faces:\n",
    "#         x, y, w, h = [ v for v in f ]\n",
    "#         cv2.rectangle(img, (x,y), (x+w,y+h), (255,255,255))\n",
    "\n",
    "#         sub_face = img[y:y+h, x:x+w]\n",
    "#         sub_faces.append(sub_face)\n",
    "#         #f_entire_name, ext = os.path.splitext(image)\n",
    "#         f_base_name = os. path. basename(image)\n",
    "#         fname, ext = os.path.splitext(f_base_name)\n",
    "#         #cv2.imwrite(fname+\"_cropped_\"+str(counter)+ext, sub_face)\n",
    "#         cv2.imwrite(os.path.join(save_path , fname+\"_cropped_\"+str(counter)+ext), \n",
    "#                     cv2.cvtColor(sub_face, cv2.COLOR_RGB2BGR))\n",
    "#         counter += 1\n",
    "#     return sub_faces, counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
